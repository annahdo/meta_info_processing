{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36dead620e64104af9042399b141eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in tokens for batch and separate:\n",
      "\"This is a short sentence.\"\n",
      "\tdiff: tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "\"This is a longer sentence that will cause the shorter sentence to have padding tokens when processed together.\"\n",
      "\tdiff: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Difference in logits for batch and separate:\n",
      "\"This is a short sentence.\"\n",
      "\tdiff: 8.49116e-06\n",
      "\tmax logits: 14.9922 and 14.9922\n",
      "\"This is a longer sentence that will cause the shorter sentence to have padding tokens when processed together.\"\n",
      "\tdiff: 1.29847e-05\n",
      "\tmax logits: 14.9688 and 14.9766\n",
      "\n",
      "\n",
      "Padded tensor rows processed separately\n",
      "\n",
      "Difference in logits for padded rows of batch and separate:\n",
      "\"This is a short sentence.\"\n",
      "\tdiff: 1.12967e-05\n",
      "\tmax logits: 14.9922 and 14.9922\n",
      "\"This is a longer sentence that will cause the shorter sentence to have padding tokens when processed together.\"\n",
      "\tdiff: 0\n",
      "\tmax logits: 14.9766 and 14.9766\n"
     ]
    }
   ],
   "source": [
    "# minimal example\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\"This is a short sentence.\", \"This is a longer sentence that will cause the shorter sentence to have padding tokens when processed together.\"]\n",
    "tokenized_batch = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "tokenized_separately = [tokenizer(sentence, return_tensors=\"pt\", padding=True) for sentence in sentences]\n",
    "\n",
    "print(f\"Difference in tokens for batch and separate:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"\\\"{s}\\\"\")\n",
    "    print(f'\\tdiff: {tokenized_batch[\"input_ids\"][i][tokenized_batch[\"attention_mask\"][i]==1] - tokenized_separately[i][\"input_ids\"][0]}')\n",
    "\n",
    "\n",
    "logits_batch = model(**tokenized_batch.to(device)).logits[:,-1, :]\n",
    "logits_separately = [model(**tokenized.to(device)).logits[:,-1, :] for tokenized in tokenized_separately]\n",
    "\n",
    "print(f\"\\nDifference in logits for batch and separate:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"\\\"{s}\\\"\")\n",
    "    print(f\"\\tdiff: {torch.mean((logits_batch[i] - logits_separately[i]).pow(2)):.6g}\")\n",
    "    print(f\"\\tmax logits: {logits_batch[i].max():.6g} and {logits_separately[i].max():.6g}\")\n",
    "\n",
    "\n",
    "# keep the padding of the tokenization but process the padded samples separately \n",
    "print(f\"\\n\\nPadded tensor rows processed separately\")\n",
    "rows_processed_separately = []\n",
    "\n",
    "for ids, attentions in zip(tokenized_batch[\"input_ids\"], tokenized_batch[\"attention_mask\"]):\n",
    "    rows_processed_separately.append(model(input_ids=ids.unsqueeze(0).to(device), attention_mask=attentions.unsqueeze(0).to(device)).logits[:,-1, :])\n",
    "\n",
    "print(f\"\\nDifference in logits for padded rows of batch and separate:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"\\\"{s}\\\"\")\n",
    "    print(f\"\\tdiff: {torch.mean((logits_separately[i] - rows_processed_separately[i]).pow(2)):.6g}\")\n",
    "    print(f\"\\tmax logits: {logits_separately[i].max():.6g} and {rows_processed_separately[i].max():.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     1,   851,   349,   264,  2485, 12271, 28723],\n",
       "        [    1,   851,   349,   264,  3774, 12271,   369,   622,  4244,   272,\n",
       "         19367, 12271,   298,   506, 12342, 16246,   739, 16244,  2553, 28723]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   851,   349,   264,  3774, 12271,   369,   622,  4244,   272,\n",
       "        19367, 12271,   298,   506, 12342, 16246,   739, 16244,  2553, 28723],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_separately[i][\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     1,   851,   349,   264,  2485, 12271, 28723]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_separately[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch['attention_mask'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.9688, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_batch[i].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import Trace\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "import utils, dataset_utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(dataset_utils)\n",
    "from utils import *\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_set('Statements1000') # load one of Statements1000, BoolQ, Burglar, FreebaseStatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6129a8872a44babebd3dcc134484fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "# load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(model, tokenizer, data, batch_size, add_padding=None):  \n",
    "    \n",
    "    device=model.device\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    all_logits = []\n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\")\n",
    "        if add_padding is not None and add_padding>0:\n",
    "            padding = int(add_padding)\n",
    "            model_inputs['input_ids'] = torch.cat([torch.full((model_inputs['input_ids'].shape[0], padding), pad_token_id, dtype=torch.long), model_inputs['input_ids']], dim=1)\n",
    "            model_inputs['attention_mask'] = torch.cat([torch.full((model_inputs['attention_mask'].shape[0], padding), 0, dtype=torch.long), model_inputs['attention_mask']], dim=1)\n",
    "\n",
    "        logits = model(**model_inputs.to(device)).logits.detach().cpu()[:, -1, :]\n",
    "        all_logits.extend(logits)\n",
    "    return torch.concatenate(all_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between batches: \n",
      "\t8-16: 1.4e-05, \n",
      "\t8-32: 1.3e-05, \n",
      "\t8-64: 1.4e-05, \n",
      "\t16-32: 1.3e-05, \n",
      "\t16-64: 1.4e-05, \n",
      "\t32-64: 1.3e-05\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset['org_data'][:100]\n",
    "\n",
    "logits8 = get_logits(model, tokenizer, test_data, 8)\n",
    "logits16 = get_logits(model, tokenizer, test_data, 16)\n",
    "logits32 = get_logits(model, tokenizer, test_data, 32)\n",
    "logits64 = get_logits(model, tokenizer, test_data, 64)\n",
    "\n",
    "# matrix of MSE between batches\n",
    "mse8_16 = torch.nn.functional.mse_loss(logits8, logits16)\n",
    "mse8_32 = torch.nn.functional.mse_loss(logits8, logits32)\n",
    "mse8_64 = torch.nn.functional.mse_loss(logits8, logits64)\n",
    "mse16_32 = torch.nn.functional.mse_loss(logits16, logits32)\n",
    "mse16_64 = torch.nn.functional.mse_loss(logits16, logits64)\n",
    "mse32_64 = torch.nn.functional.mse_loss(logits32, logits64)\n",
    "\n",
    "print(f\"MSE between batches: \\n\\t8-16: {mse8_16:.2g}, \\n\\t8-32: {mse8_32:.2g}, \\n\\t8-64: {mse8_64:.2g}, \\n\\t16-32: {mse16_32:.2g}, \\n\\t16-64: {mse16_64:.2g}, \\n\\t32-64: {mse32_64:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between batches with padding 0: \n",
      "\t8-8_padded: 0, \n",
      "\t16-16_padded: 0\n",
      "MSE between batches with padding 1: \n",
      "\t8-8_padded: 1.3e-05, \n",
      "\t16-16_padded: 1.4e-05\n",
      "MSE between batches with padding 5: \n",
      "\t8-8_padded: 1.6e-05, \n",
      "\t16-16_padded: 1.5e-05\n",
      "MSE between batches with padding 10: \n",
      "\t8-8_padded: 1.4e-05, \n",
      "\t16-16_padded: 1.5e-05\n",
      "MSE between batches with padding 50: \n",
      "\t8-8_padded: 1.4e-05, \n",
      "\t16-16_padded: 1.5e-05\n",
      "MSE between batches with padding 100: \n",
      "\t8-8_padded: 1.5e-05, \n",
      "\t16-16_padded: 1.6e-05\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset['org_data'][:100]\n",
    "\n",
    "for add_padding in [0,1,5,10,50,100]:\n",
    "\n",
    "    logits8_padded = get_logits(model, tokenizer, test_data, 8, add_padding=add_padding)\n",
    "    logits16_padded = get_logits(model, tokenizer, test_data, 16, add_padding=add_padding)\n",
    "    #logits32_padded = get_logits(model, tokenizer, test_data, 32, add_padding=add_padding)\n",
    "    #logits64_padded = get_logits(model, tokenizer, test_data, 64, add_padding=add_padding)\n",
    "\n",
    "    # matrix of MSE between batches\n",
    "    mse8_8_padded = torch.nn.functional.mse_loss(logits8_padded, logits8)\n",
    "    mse16_16_padded = torch.nn.functional.mse_loss(logits16_padded, logits16)\n",
    "    #mse8_32_padded = torch.nn.functional.mse_loss(logits8, logits32)\n",
    "    #mse8_64_padded = torch.nn.functional.mse_loss(logits8, logits64)\n",
    "    #mse16_32_padded = torch.nn.functional.mse_loss(logits16, logits32)\n",
    "    #mse16_64_padded = torch.nn.functional.mse_loss(logits16, logits64)\n",
    "    #mse32_64_padded = torch.nn.functional.mse_loss(logits32, logits64)\n",
    "\n",
    "\n",
    "    print(f\"MSE between batches with padding {add_padding}: \\n\\t8-8_padded: {mse8_8_padded:.2g}, \\n\\t16-16_padded: {mse16_16_padded:.2g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "Padding changes model output, when using different batch sizes, more or less padding for the same sentence is possible. Sometimes the logits are changed so much that the argmax is actually a different token. this is why it can result in mismatches of tokens being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data = test_data\n",
    "    batch_size=64\n",
    "    device=model.device\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    all_logits = []\n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\")\n",
    "        logits = model(**model_inputs.to(device)).logits.detach().cpu()[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    generated_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\").to(device)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()\n",
    "        generated_tokens.extend(generated_ids[:, model_inputs.input_ids.shape[1]:])\n",
    "    \n",
    "    return torch.tensor(generated_tokens)\n",
    "\n",
    "def generate_batch_only_tokenizer(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    generated_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\").to(device)\n",
    "        for input_ids, attention_mask in zip(model_inputs['input_ids'], model_inputs['attention_mask']):\n",
    "            generated_ids = model.generate(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0), max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()\n",
    "            generated_tokens.extend(generated_ids[:, model_inputs.input_ids.shape[1]:])\n",
    "    return torch.tensor(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "\n",
      "\n",
      "tensor([121, 999])\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Edmund Neupert, performing on the\n",
      "tensor(13221) tensor(7454)\n",
      "piano ther\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Bhaktisiddhanta Saraswati, who has a citizenship of\n",
      "tensor(475) tensor(15798)\n",
      "J Mars\n"
     ]
    }
   ],
   "source": [
    "data = dataset[\"lie_scenario\"]\n",
    "\n",
    "lie_tokens64 = generate(model, tokenizer, data, batch_size=64)\n",
    "lie_tokens32 = generate(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "print((lie_tokens64==lie_tokens32).all())\n",
    "print('\\n')\n",
    "indices = torch.where(lie_tokens64!=lie_tokens32)[0]\n",
    "print(indices)\n",
    "for idx in indices:\n",
    "    print(data[idx])\n",
    "    print(lie_tokens64[idx], lie_tokens32[idx])\n",
    "    print(tokenizer.decode(lie_tokens64[idx]), tokenizer.decode(lie_tokens32[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tokens are produced, depending on what batch size is used. This should not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "\n",
      "\n",
      "tensor([365])\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Mitsubishi Corporation's headquarters are in\n",
      "tensor(272) tensor(1639)\n",
      "the fact\n"
     ]
    }
   ],
   "source": [
    "lie_tokens64_ = generate_batch_only_tokenizer(model, tokenizer, data, batch_size=64)\n",
    "lie_tokens32_ = generate_batch_only_tokenizer(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "print((lie_tokens64_==lie_tokens32_).all())\n",
    "print('\\n')\n",
    "indices = torch.where(lie_tokens64_!=lie_tokens32_)[0]\n",
    "print(indices)\n",
    "for idx in indices:\n",
    "    print(data[idx])\n",
    "    print(lie_tokens64_[idx], lie_tokens32_[idx])\n",
    "    print(tokenizer.decode(lie_tokens64_[idx]), tokenizer.decode(lie_tokens32_[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I process the lines separately using generate I get a different result. Why? this really should not matter. There should be zero connection between different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_logit(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    max_logit_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\")\n",
    "        argmax = model(**model_inputs.to(device)).logits.detach().cpu()[:, -1, :].argmax(dim=-1)\n",
    "        max_logit_tokens.extend(argmax)\n",
    "    return torch.tensor(max_logit_tokens)\n",
    "\n",
    "def get_max_logit_batch_only_tokenizer(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    max_logit_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\").to(device)\n",
    "        for input_ids, attention_mask in zip(model_inputs['input_ids'], model_inputs['attention_mask']):\n",
    "            argmax = model(input_ids=input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0)).logits.detach().cpu()[:, -1, :].argmax(dim=-1)\n",
    "            max_logit_tokens.extend(argmax)\n",
    "    return torch.tensor(max_logit_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "\n",
      "\n",
      "tensor([121, 363, 999])\n",
      "\n",
      "\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Edmund Neupert, performing on the\n",
      "tensor(13221) tensor(7454)\n",
      "piano ther\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Gaetano Moroni passed away in\n",
      "tensor(28705) tensor(272)\n",
      " the\n",
      "<|system|>You will complete the sentence with intentionally false information.</s><|assistant|>Bhaktisiddhanta Saraswati, who has a citizenship of\n",
      "tensor(15798) tensor(475)\n",
      "Mars J\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "max_logit_tokens64 = get_max_logit(model, tokenizer, data, batch_size=64)\n",
    "max_logit_tokens32 = get_max_logit(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "print((max_logit_tokens64==max_logit_tokens32).all())\n",
    "print('\\n')\n",
    "indices = torch.where(max_logit_tokens64!=max_logit_tokens32)[0]\n",
    "print(indices)\n",
    "print('\\n')\n",
    "for idx in indices:\n",
    "    print(data[idx])\n",
    "    print(max_logit_tokens64[idx], max_logit_tokens32[idx])\n",
    "    print(tokenizer.decode(max_logit_tokens64[idx]), tokenizer.decode(max_logit_tokens32[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "\n",
      "\n",
      "tensor([], dtype=torch.int64)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "max_logit_tokens64_ = get_max_logit_batch_only_tokenizer(model, tokenizer, data, batch_size=64)\n",
    "max_logit_tokens32_ = get_max_logit_batch_only_tokenizer(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "print((max_logit_tokens64_==max_logit_tokens32_).all())\n",
    "print('\\n')\n",
    "indices = torch.where(max_logit_tokens64_!=max_logit_tokens32_)[0]\n",
    "print(indices)\n",
    "print('\\n')\n",
    "for idx in indices:\n",
    "    print(data[idx])\n",
    "    print(max_logit_tokens64_[idx], max_logit_tokens32_[idx])\n",
    "    print(tokenizer.decode(max_logit_tokens64_[idx]), tokenizer.decode(max_logit_tokens32_[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here everything matches. I'm giving up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor([363, 999])\n",
      "tensor(False)\n",
      "tensor([999])\n"
     ]
    }
   ],
   "source": [
    "print((max_logit_tokens64==lie_tokens64).all())\n",
    "print(torch.where(max_logit_tokens64!=lie_tokens64)[0])\n",
    "print((max_logit_tokens32==lie_tokens32).all())\n",
    "print(torch.where(max_logit_tokens32!=lie_tokens32)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
