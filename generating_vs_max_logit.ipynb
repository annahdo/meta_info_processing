{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import Trace\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "import utils, dataset_utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(dataset_utils)\n",
    "from utils import *\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_set('Statements1000') # load one of Statements1000, BoolQ, Burglar, FreebaseStatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798fe99266284fa5a5b542762ca62231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "# load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    generated_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\").to(device)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()\n",
    "        generated_tokens.extend(generated_ids[:, model_inputs.input_ids.shape[1]:])\n",
    "    \n",
    "    return torch.tensor(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "data = dataset[\"lie_scenario\"][-257:-193]\n",
    "lie_tokens64 = generate(model, tokenizer, data, batch_size=64)\n",
    "\n",
    "lie_tokens32 = generate(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "lie_tokens8 = generate(model, tokenizer, data, batch_size=8)\n",
    "\n",
    "print((lie_tokens64==lie_tokens32).all())\n",
    "print((lie_tokens64==lie_tokens8).all())\n",
    "print((lie_tokens8==lie_tokens32).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20687]) tensor([2952])\n"
     ]
    }
   ],
   "source": [
    "print(lie_tokens64[torch.where(lie_tokens64!=lie_tokens32)], lie_tokens32[torch.where(lie_tokens64!=lie_tokens32)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tokens are produced, depending on what batch size is used. This should not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_logit(model, tokenizer, data, batch_size):\n",
    "    device=model.device\n",
    "    max_logit_tokens = []\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    for batch in batchify(data, batch_size):\n",
    "        model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\")\n",
    "        for input_ids, attention_mask in zip(model_inputs[\"input_ids\"], model_inputs[\"attention_mask\"]): # cuda goes out of memory if i try to process a large batch\n",
    "            input_ids = input_ids.unsqueeze(0).to(device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "            argmax = model(input_ids, attention_mask=attention_mask).logits.detach().cpu()[:, -1, :].argmax(dim=-1)\n",
    "            max_logit_tokens.extend(argmax)\n",
    "    return torch.tensor(max_logit_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "max_logit_tokens64 = get_max_logit(model, tokenizer, data, batch_size=64)\n",
    "\n",
    "max_logit_tokens32 = get_max_logit(model, tokenizer, data, batch_size=32)\n",
    "\n",
    "max_logit_tokens8 = get_max_logit(model, tokenizer, data, batch_size=8)\n",
    "\n",
    "print((max_logit_tokens64==max_logit_tokens32).all())\n",
    "print((max_logit_tokens64==max_logit_tokens8).all())\n",
    "print((max_logit_tokens8==max_logit_tokens32).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print((max_logit_tokens64==lie_tokens64).all())\n",
    "print((max_logit_tokens32==lie_tokens32).all())\n",
    "print((max_logit_tokens8==lie_tokens8).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=model.device\n",
    "generated_tokens = []\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for batch in batchify(data, 64):\n",
    "    model_inputs = tokenizer(list(batch), padding=True, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()\n",
    "    generated_tokens.extend(generated_ids[:, model_inputs.input_ids.shape[1]:])\n",
    "generated_tokens = torch.tensor(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.where(lie_tokens64!=lie_tokens32)[0][0]\n",
    "\n",
    "print(generated_tokens[idx])\n",
    "print(lie_tokens64[idx])\n",
    "print(lie_tokens32[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20687)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = model_inputs['input_ids'][:, :]\n",
    "attention = model_inputs['attention_mask'][:, :]\n",
    "out = model.generate(input_ids=inputs.to(device),attention_mask=attention.to(device), max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()[:,-1]\n",
    "out[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2952)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = model_inputs['input_ids'][:idx+1, :]\n",
    "attention = model_inputs['attention_mask'][:idx+1, :]\n",
    "out = model.generate(input_ids=inputs.to(device),attention_mask=attention.to(device), max_new_tokens=1, pad_token_id=pad_token_id, do_sample=False, use_cache=True).detach().cpu()[:,-1]\n",
    "out[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
