{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baukit import Trace\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "import utils, dataset_utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(dataset_utils)\n",
    "from utils import *\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folders plots, data results if they dont exist\n",
    "for folder in ['plots', 'data', 'results']:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# model_name = \"huggyllama/llama-7b\"\n",
    "# load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress system output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_success_rates(model, tokenizer, dataset_name, formats, max_new_tokens=5, batch_size=64):\n",
    "    lie_formats = formats['lie_formats']\n",
    "    truth_formats = formats['truth_formats']\n",
    "\n",
    "    success_lies = {}\n",
    "    success_truths = {}\n",
    "\n",
    "    print(f\"Testing {len(lie_formats)} lie formats for {dataset_name}...\\n\")\n",
    "    for lie_format in lie_formats:\n",
    "        dataset = load_data_set(dataset_name, lie_format=lie_format)\n",
    "        _, answer_tokens_lie = generate_tokens(model, tokenizer, dataset['lie_scenario'], \n",
    "                                                            max_new_tokens=max_new_tokens, batch_size=batch_size, do_sample=False)\n",
    "\n",
    "        success_lie = check_answer(tokenizer, answer_tokens_lie, dataset['true_answer'], batch_size=batch_size)\n",
    "        success = 100-np.mean(success_lie)*100\n",
    "        success_lies[lie_format] = success\n",
    "\n",
    "        print(f\"{lie_format}: \\n\\t success_rate: {success:.2f}%\")\n",
    "\n",
    "\n",
    "    print(f\"\\nTesting {len(truth_formats)} truth formats for {dataset_name}...\\n\")\n",
    "    for truth_format in truth_formats:\n",
    "        dataset = load_data_set(dataset_name, truth_format=truth_format)\n",
    "        _, answer_tokens_truth = generate_tokens(model, tokenizer, dataset['truth_scenario'], \n",
    "                                                                max_new_tokens=max_new_tokens, batch_size=batch_size, do_sample=False)\n",
    "\n",
    "        success_truth = check_answer(tokenizer, answer_tokens_truth, dataset['true_answer'], batch_size=batch_size)\n",
    "        success = np.mean(success_truth)*100\n",
    "        success_truths[truth_format] = success\n",
    "        print(f\"{truth_format}: \\n\\t success_rate: {success:.2f}%\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Sorted by success rate: \\n\")\n",
    "\n",
    "    print_success_rates(success_lies, success_truths, dataset_name)\n",
    "\n",
    "\n",
    "def print_success_rates(success_lies, success_truths, dataset_name):\n",
    "    # sort formats by success rate\n",
    "    success_lies = dict(sorted(success_lies.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(f\"Success rates for lie formats for dataset {dataset_name}:\")\n",
    "    for lie_format, success in success_lies.items():\n",
    "        print(f\"{lie_format}\\n\\t success_rate: {success:.2f}%\")\n",
    "\n",
    "    success_truths = dict(sorted(success_truths.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(f\"Success rates for truth formats for dataset {dataset_name}:\")\n",
    "    for truth_format, success in success_truths.items():\n",
    "        print(f\"{truth_format}\\n\\t success_rate: {success:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 5\n",
    "batch_size = 64\n",
    "\n",
    "# define prompt_formats\n",
    "with open('data/formats_statements.json') as json_file:\n",
    "    formats_statements = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_success_rates(model, tokenizer, 'Questions1000', formats_statements, max_new_tokens=max_new_tokens, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_success_rates(model, tokenizer, 'FreebaseStatements', formats_statements, max_new_tokens=max_new_tokens, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
